{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOP9xaHaYfLmd7XW1TxeA5H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riandongwoo/AI_tutorial_code/blob/main/Hands-on-session5/Hands-on-session5-BO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI-driven Materials Development"
      ],
      "metadata": {
        "id": "irNvUvBqBQG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian Optimization\n",
        "Peter I. Frazier arXiv:1807.02811v1\n",
        "\n",
        "Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quanti es the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function de ned from this surrogate to decide where to sample.\n",
        "\n",
        "Bayesian optimization (BayesOpt) is a class of machine-learning-based optimization methods focused on solving the problem\n",
        "$$max_{x \\subseteq A} f(x)$$\n",
        "where the feasible set and objective function typically have the following properties\n",
        "\n",
        "- The input x is in $R^d$ for a value of d that is not too large. Typically d 20 in most successful applications of BayesOpt.\n",
        "\n",
        "- The feasible set A is a simple set, in which it is easy to assess membership. Typically A is a hyper-rectangle {$x \\subseteq R^d$ : $a_i \\le x_i \\le b_i$} or the d-dimensional simplex {$x \\subseteq R_d$ : $Î£_{x_i} = 1$} . Later (Section 5) we will relax this assumption.\n",
        "\n",
        "- The objective function f is continuous. This will typically be required to model f using Gaussian process regression.\n",
        "\n",
        "- f is expensive to evaluate in the sense that the number of evaluations that may be performed is limited, typically to a few hundred. This limitation typically arises because each evaluation takes a substantial amount of time (typically hours), but may also occur because each evaluation bears a monetary cost (e.g., from purchasing cloud computing power, or buying laboratory materials), or an opportunity cost (e.g., if evaluating f requires asking a human subject questions who will tolerate only a limited number).\n",
        "\n",
        "- f lacks known special structure like concavity or linearity that would make it easy to optimize using techniques that leverage such structure to improve e ciency. We summarize this by saying f is a black box.\n",
        "\n",
        "- When we evaluate f, we observe only f(x) and no rst- or second-order derivatives. This prevents the application of rst- and second-order methods like gradient descent, Newtons method, or quasiNewton methods. We refer to problems with this property as derivative-free .\n",
        "\n",
        "- Our focus is on nding a global rather than local optimum."
      ],
      "metadata": {
        "id": "LiWhY_bYBTSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8Gkibt9SFe7A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNJX2A64AN-Z"
      },
      "outputs": [],
      "source": []
    }
  ]
}